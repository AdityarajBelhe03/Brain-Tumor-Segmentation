{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be801742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
    "from metrics import dice_loss, dice_coef, iou\n",
    "from train import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783336f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "H = 128\n",
    "W = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d72ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Creating a directory \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c02d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(image, mask, y_pred, save_image_path):\n",
    "    ## i - m - y\n",
    "    line = np.ones((H, 10, 3)) * 128\n",
    "\n",
    "    \"\"\" Mask \"\"\"\n",
    "    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n",
    "\n",
    "    \"\"\" Predicted Mask \"\"\"\n",
    "    y_pred = np.expand_dims(y_pred, axis=-1)    ## (512, 512, 1)\n",
    "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1)  ## (512, 512, 3)\n",
    "    y_pred = y_pred * 255\n",
    "\n",
    "    cat_images = np.concatenate([image, line, mask, line, y_pred], axis=1)\n",
    "    cv2.imwrite(save_image_path, cat_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "110b024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Seeding \"\"\"\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b051f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Directory for storing files \"\"\"\n",
    "create_dir(\"results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23af208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loading model \"\"\"\n",
    "with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef, 'dice_loss': dice_loss}):\n",
    "    model = tf.keras.models.load_model(\"files/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9410e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0 - 0\n"
     ]
    }
   ],
   "source": [
    "    \"\"\" Load the dataset \"\"\"\n",
    "    test_x = sorted(glob(os.path.join(\"test_data\", \"test\", \"image\", \"*\")))\n",
    "    test_y = sorted(glob(os.path.join(\"test_data\", \"test\", \"mask\", \"*\")))\n",
    "    print(f\"Test: {len(test_x)} - {len(test_y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522fd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \"\"\" Evaluation and Prediction \"\"\"\n",
    "    SCORE = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e326daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "C:\\Users\\upratham\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\upratham\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m score \u001b[38;5;241m=\u001b[39m [s[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m SCORE]\n\u001b[0;32m     40\u001b[0m score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(score, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJaccard: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "\n",
    "    for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
    "        \"\"\" Extract the name \"\"\"\n",
    "        name = x.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "\n",
    "        \"\"\" Reading the image \"\"\"\n",
    "        image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "        x = image/255.0\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "\n",
    "        \"\"\" Reading the mask \"\"\"\n",
    "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "        y = mask/255.0\n",
    "        y = y > 0.5\n",
    "        y = y.astype(np.int32)\n",
    "\n",
    "        \"\"\" Prediction \"\"\"\n",
    "        y_pred = model.predict(x)[0]\n",
    "        y_pred = np.squeeze(y_pred, axis=-1)\n",
    "        y_pred = y_pred > 0.5\n",
    "        y_pred = y_pred.astype(np.int32)\n",
    "\n",
    "        \"\"\" Saving the prediction \"\"\"\n",
    "        save_image_path = f\"results\\\\{name}.png\"\n",
    "        save_results(image, mask, y_pred, save_image_path)\n",
    "\n",
    "        \"\"\" Flatten the array \"\"\"\n",
    "        y = y.flatten()\n",
    "        y_pred = y_pred.flatten()\n",
    "\n",
    "        \"\"\" Calculating the metrics values \"\"\"\n",
    "        acc_value = accuracy_score(y, y_pred)\n",
    "        f1_value = f1_score(y, y_pred, labels=[0, 1], average=\"binary\", zero_division=1)\n",
    "        jac_value = jaccard_score(y, y_pred, labels=[0, 1], average=\"binary\", zero_division=1)\n",
    "        recall_value = recall_score(y, y_pred, labels=[0, 1], average=\"binary\", zero_division=1)\n",
    "        precision_value = precision_score(y, y_pred, labels=[0, 1], average=\"binary\", zero_division=1)\n",
    "        SCORE.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n",
    "\n",
    "    \"\"\" Metrics values \"\"\"\n",
    "    score = [s[1:]for s in SCORE]\n",
    "    score = np.mean(score, axis=0)\n",
    "    print(f\"Accuracy: {score[0]:0.5f}\")\n",
    "    print(f\"F1: {score[1]:0.5f}\")\n",
    "    print(f\"Jaccard: {score[2]:0.5f}\")\n",
    "    print(f\"Recall: {score[3]:0.5f}\")\n",
    "    print(f\"Precision: {score[4]:0.5f}\")\n",
    "\n",
    "    df = pd.DataFrame(SCORE, columns=[\"Image\", \"Accuracy\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"])\n",
    "    df.to_csv(\"files/score.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "838a6b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96336\n",
      "F1: 0.73664\n",
      "Jaccard: 0.65760\n",
      "Recall: 0.74333\n",
      "Precision: 0.87694\n"
     ]
    }
   ],
   "source": [
    "    print(f\"Accuracy: {score[0]:0.5f}\")\n",
    "    print(f\"F1: {score[1]:0.5f}\")\n",
    "    print(f\"Jaccard: {score[2]:0.5f}\")\n",
    "    print(f\"Recall: {score[3]:0.5f}\")\n",
    "    print(f\"Precision: {score[4]:0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65571fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
